{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "#Download CodonBERT_Sanofi model and add path\n",
    "model = AutoModelForMaskedLM.from_pretrained('../CodonBERT_Sanofi/codonbert_models/codonbert')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sequences from fastaa file\n",
    "def read_fasta(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    sequences = {}\n",
    "    sequence_name = None\n",
    "    sequence = ''\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('>'):\n",
    "            if sequence_name:  # save the sequence when we encounter a new header\n",
    "                sequences[sequence_name] = sequence\n",
    "                sequence = ''\n",
    "            sequence_name = line.strip()\n",
    "        else:\n",
    "            sequence += line.strip()\n",
    "\n",
    "    # Save the last sequence\n",
    "    if sequence_name and sequence:\n",
    "        sequences[sequence_name] = sequence\n",
    "\n",
    "    return sequences\n",
    "\n",
    "fasta_file = 'exp_nature_gene.faa'\n",
    "sequences = read_fasta(fasta_file)\n",
    "sequence = list(sequences.values())[0]\n",
    "#print(sequence)\n",
    "#sequence = 'AUGCCAAACACCCUGGCAUGCCCC'\n",
    "# Format sequence so it's compatible with CodonBert (sepaarte codnos by space)\n",
    "codons = [sequence[i:i+3] for i in range(0, len(sequence), 3)]\n",
    "spaced_sequence = ' '.join(codons)\n",
    "print(spaced_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example: Get model outputs for a sequence with one masked Codon\n",
    "from tokenizer import get_tokenizer\n",
    "import torch\n",
    "seq = '[MASK] AAU GAU ACG GAA GCG AUC'\n",
    "tokenizer = get_tokenizer()\n",
    "input_ids = tokenizer.encode(seq).ids\n",
    "input_ids = torch.tensor([input_ids], dtype=torch.int64)  # batch_size = 1\n",
    "outputs = model(input_ids)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output logits shape is ([batch size, number of tokens, vocab_size])\n",
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example: Predict the masked codon in example sequence and compare with real\n",
    "import numpy as np\n",
    "seq_codons = seq.split()\n",
    "idx = seq_codons.index('[MASK]')\n",
    "print(idx)\n",
    "torch.argmax(outputs.logits[0,idx+1])\n",
    "print('real: ', spaced_sequence.split()[idx])\n",
    "#tokenizer.decode([np.argmax(outputs.logits[0,idx].detach().numpy())])\n",
    "print('predicted: ', tokenizer.decode([np.argmax(outputs.logits[0,idx+1].detach().numpy())]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask each codon one by one in a sequence and predict the masked codon. \n",
    "correct_predictions = 0\n",
    "\n",
    "for idx in range(len(codons)):\n",
    "    masked_codon = codons[idx]\n",
    "    masked_seq = ' '.join(codons[:idx] + ['[MASK]'] + codons[idx+1:])\n",
    "    masked_input_ids = tokenizer.encode(masked_seq).ids\n",
    "    masked_input_ids = torch.tensor([masked_input_ids], dtype=torch.int64)  # batch_size = 1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(masked_input_ids)\n",
    "    \n",
    "    # Get the index of the masked token in the logits output\n",
    "    mask_token_index = masked_input_ids[0].tolist().index(tokenizer.token_to_id('[MASK]'))\n",
    "    predicted_token_id = torch.argmax(outputs.logits[0, mask_token_index])\n",
    "    predicted_codon = tokenizer.decode([predicted_token_id])\n",
    "    \n",
    "    print(f\"Masked sequence: {masked_seq}\")\n",
    "    print(f\"Real codon: {masked_codon}\")\n",
    "    print(f\"Predicted codon: {predicted_codon}\")\n",
    "    \n",
    "    if masked_codon == predicted_codon:\n",
    "        correct_predictions += 1\n",
    "\n",
    "total_codons = len(codons)\n",
    "accuracy = correct_predictions / total_codons\n",
    "\n",
    "print(f\"Total correct predictions: {correct_predictions}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find accuarcy of predictions by comparing the real and predicted masked codons. Include synonymous codons\n",
    "from tokenizer import get_tokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Assuming the sequence and the model are already defined\n",
    "sequence = 'AUGAAUGAUACGGAAGCGAUC'\n",
    "codons = [sequence[i:i+3] for i in range(0, len(sequence), 3)]\n",
    "spaced_sequence = ' '.join(codons)\n",
    "print('Original sequence:', spaced_sequence)\n",
    "\n",
    "# Codon table mapping\n",
    "codon_table = {\n",
    "    'G': ['GGU', 'GGC', 'GGA', 'GGG'],\n",
    "    'A': ['GCU', 'GCC', 'GCA', 'GCG'],\n",
    "    'V': ['GUU', 'GUC', 'GUA', 'GUG'],\n",
    "    'L': ['CUU', 'CUC', 'CUA', 'CUG', 'UUA', 'UUG'],\n",
    "    'I': ['AUU', 'AUC', 'AUA'],\n",
    "    'P': ['CCU', 'CCA', 'CCG', 'CCC'],\n",
    "    'F': ['UUU', 'UUC'],\n",
    "    'Y': ['UAU', 'UAC'],\n",
    "    'W': ['UGG'],\n",
    "    'S': ['UCU', 'UCA', 'UCC', 'UCG', 'AGU', 'AGC'],\n",
    "    'T': ['ACU', 'ACC', 'ACG', 'ACA'],\n",
    "    'M': ['AUG'],\n",
    "    'C': ['UGU', 'UGC'],\n",
    "    'N': ['AAU', 'AAC'],\n",
    "    'Q': ['CAA', 'CAG'],\n",
    "    'D': ['GAU', 'GAC'],\n",
    "    'E': ['GAA', 'GAG'],\n",
    "    'K': ['AAA', 'AAG'],\n",
    "    'R': ['CGU', 'CGC', 'CGG', 'CGA', 'AGA', 'AGG'],\n",
    "    'H': ['CAU', 'CAC'],\n",
    "    'X': ['UAA', 'UAG', 'UGA']\n",
    "}\n",
    "\n",
    "# Function to get amino acid for a given codon\n",
    "def get_amino_acid(codon):\n",
    "    for aa, codons in codon_table.items():\n",
    "        if codon in codons:\n",
    "            return aa\n",
    "    return None\n",
    "\n",
    "# Initialize tokenizer and model (assuming model is already defined)\n",
    "tokenizer = get_tokenizer()\n",
    "seq = ' '.join(codons)\n",
    "\n",
    "# Encode the sequence\n",
    "input_ids = tokenizer.encode(seq).ids\n",
    "input_ids = torch.tensor([input_ids], dtype=torch.int64)  # batch_size = 1\n",
    "\n",
    "# Mask each codon one by one and predict\n",
    "correct_predictions = 0\n",
    "\n",
    "for idx in range(len(codons)):\n",
    "    masked_codon = codons[idx]\n",
    "    masked_seq = ' '.join(codons[:idx] + ['[MASK]'] + codons[idx+1:])\n",
    "    masked_input_ids = tokenizer.encode(masked_seq).ids\n",
    "    masked_input_ids = torch.tensor([masked_input_ids], dtype=torch.int64)  # batch_size = 1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(masked_input_ids)\n",
    "    \n",
    "    # Get the index of the masked token in the logits output\n",
    "    mask_token_index = masked_input_ids[0].tolist().index(tokenizer.token_to_id('[MASK]'))\n",
    "    predicted_token_id = torch.argmax(outputs.logits[0, mask_token_index])\n",
    "    predicted_codon = tokenizer.decode([predicted_token_id])\n",
    "    \n",
    "    real_amino_acid = get_amino_acid(masked_codon)\n",
    "    predicted_amino_acid = get_amino_acid(predicted_codon)\n",
    "\n",
    "    print(f\"Masked sequence: {masked_seq}\")\n",
    "    print(f\"Real codon: {masked_codon} (Amino acid: {real_amino_acid})\")\n",
    "    print(f\"Predicted codon: {predicted_codon} (Amino acid: {predicted_amino_acid})\")\n",
    "    \n",
    "    if real_amino_acid == predicted_amino_acid:\n",
    "        correct_predictions += 1\n",
    "\n",
    "total_codons = len(codons)\n",
    "accuracy = correct_predictions / total_codons\n",
    "\n",
    "print(f\"Total correct predictions: {correct_predictions}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
